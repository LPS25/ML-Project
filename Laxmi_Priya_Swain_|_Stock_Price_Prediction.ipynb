{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [
        "vncDsAP0Gaoa",
        "FJNUwmbgGyua",
        "w6K7xa23Elo4",
        "yQaldy8SH6Dl",
        "mDgbUHAGgjLW",
        "O_i_v8NEhb9l",
        "HhfV-JJviCcP",
        "Y3lxredqlCYt",
        "3RnN4peoiCZX",
        "x71ZqKXriCWQ",
        "7hBIi_osiCS2",
        "JlHwYmJAmNHm",
        "35m5QtbWiB9F",
        "PoPl-ycgm1ru",
        "H0kj-8xxnORC",
        "nA9Y7ga8ng1Z",
        "PBTbrJXOngz2",
        "u3PMJOP6ngxN",
        "dauF4eBmngu3",
        "bKJF3rekwFvQ",
        "MSa1f5Uengrz",
        "GF8Ens_Soomf",
        "0wOQAZs5pc--",
        "K5QZ13OEpz2H",
        "lQ7QKXXCp7Bj",
        "448CDAPjqfQr",
        "KSlN3yHqYklG",
        "t6dVpIINYklI",
        "ijmpgYnKYklI",
        "-JiQyfWJYklI",
        "EM7whBJCYoAo",
        "fge-S5ZAYoAp",
        "85gYPyotYoAp",
        "RoGjAbkUYoAp",
        "4Of9eVA-YrdM",
        "iky9q4vBYrdO",
        "F6T5p64dYrdO",
        "y-Ehk30pYrdP",
        "bamQiAODYuh1",
        "QHF8YVU7Yuh3",
        "GwzvFGzlYuh3",
        "qYpmQ266Yuh3",
        "OH-pJp9IphqM",
        "bbFf2-_FphqN",
        "_ouA3fa0phqN",
        "Seke61FWphqN",
        "PIIx-8_IphqN",
        "t27r6nlMphqO",
        "r2jJGEOYphqO",
        "b0JNsNcRphqO",
        "BZR9WyysphqO",
        "jj7wYXLtphqO",
        "eZrbJ2SmphqO",
        "rFu4xreNphqO",
        "YJ55k-q6phqO",
        "gCFgpxoyphqP",
        "OVtJsKN_phqQ",
        "lssrdh5qphqQ",
        "U2RJ9gkRphqQ",
        "1M8mcRywphqQ",
        "tgIPom80phqQ",
        "JMzcOPDDphqR",
        "x-EpHcCOp1ci",
        "X_VqEhTip1ck",
        "8zGJKyg5p1ck",
        "PVzmfK_Ep1ck",
        "n3dbpmDWp1ck",
        "ylSl6qgtp1ck",
        "ZWILFDl5p1ck",
        "M7G43BXep1ck",
        "Ag9LCva-p1cl",
        "E6MkPsBcp1cl",
        "2cELzS2fp1cl",
        "3MPXvC8up1cl",
        "NC_X3p0fY2L0",
        "UV0SzAkaZNRQ",
        "YPEH6qLeZNRQ",
        "q29F0dvdveiT",
        "EXh0U9oCveiU",
        "22aHeOlLveiV",
        "g-ATYxFrGrvw",
        "Yfr_Vlr8HBkt",
        "8yEUt7NnHlrM",
        "tEA2Xm5dHt1r",
        "I79__PHVH19G",
        "Ou-I18pAyIpj",
        "fF3858GYyt-u",
        "4_0_7-oCpUZd",
        "hwyV_J3ipUZe",
        "3yB-zSqbpUZe",
        "dEUvejAfpUZe",
        "Fd15vwWVpUZf",
        "bn_IUdTipZyH",
        "49K5P_iCpZyH",
        "Nff-vKELpZyI",
        "kLW572S8pZyI",
        "dWbDXHzopZyI",
        "yLjJCtPM0KBk",
        "xiyOF9F70UgQ",
        "7wuGOrhz0itI",
        "id1riN9m0vUs",
        "578E2V7j08f6",
        "89xtkJwZ18nB",
        "67NQN5KX2AMe",
        "Iwf50b-R2tYG",
        "GMQiZwjn3iu7",
        "WVIkgGqN3qsr",
        "XkPnILGE3zoT",
        "Hlsf0x5436Go",
        "mT9DMSJo4nBL",
        "c49ITxTc407N",
        "OeJFEK0N496M",
        "9ExmJH0g5HBk",
        "cJNqERVU536h",
        "k5UmGsbsOxih",
        "T0VqWOYE6DLQ",
        "qBMux9mC6MCf",
        "-oLEiFgy-5Pf",
        "C74aWNz2AliB",
        "2DejudWSA-a0",
        "pEMng2IbBLp7",
        "rAdphbQ9Bhjc",
        "TNVZ9zx19K6k",
        "nqoHp30x9hH9",
        "rMDnDkt2B6du",
        "yiiVWRdJDDil",
        "1UUpS68QDMuG",
        "kexQrXU-DjzY",
        "T5CmagL3EC8N",
        "BhH2vgX9EjGr",
        "qjKvONjwE8ra",
        "P1XJ9OREExlT",
        "VFOzZv6IFROw",
        "TIqpNgepFxVj",
        "VfCC591jGiD4",
        "OB4l2ZhMeS1U",
        "ArJBuiUVfxKd",
        "4qY1EAkEfxKe",
        "PiV4Ypx8fxKe",
        "TfvqoZmBfxKf",
        "dJ2tPlVmpsJ0",
        "JWYfwnehpsJ1",
        "-jK_YjpMpsJ2",
        "HAih1iBOpsJ2",
        "zVGeBEFhpsJ2",
        "bmKjuQ-FpsJ3",
        "Fze-IPXLpx6K",
        "7AN1z2sKpx6M",
        "9PIHJqyupx6M",
        "_-qAgymDpx6N",
        "Z-hykwinpx6N",
        "h_CCil-SKHpo",
        "cBFFvTBNJzUa",
        "HvGl1hHyA_VK",
        "EyNgTHvd2WFk",
        "KH5McJBi2d8v",
        "iW_Lq9qf2h6X",
        "-Kee-DAl2viO",
        "gCX9965dhzqZ",
        "gIfDvo9L0UH2"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/LPS25/ML-Project/blob/main/Laxmi_Priya_Swain_%7C_Stock_Price_Prediction.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Project Name**    - Machine Learning Project On Stock Price Prediction\n",
        "\n"
      ],
      "metadata": {
        "id": "vncDsAP0Gaoa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Project Summary -**"
      ],
      "metadata": {
        "id": "FJNUwmbgGyua"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Machine learning proves immensely helpful in many industries in automating tasks that earlier required human labor one such application of ML is predicting whether a particular trade will be profitable or not.\n",
        "\n",
        "In this project, we will learn how to predict a signal that indicates whether buying a particular stock will be helpful or not by using ML.\n",
        "\n",
        "Let’s start by importing some libraries which will be used for various purposes which will be explained later in this project.\n",
        "\n",
        "Execute an end-to-end data science project by following the below steps:\n",
        "\n",
        "Step 1: **Define the Problem Statement**\n",
        "\n",
        "Understand the industry and categorize the problem type (Supervised, Unsupervised, Semi, etc.).\n",
        "Comprehend the business objective and desired outcomes.\n",
        "Identify constraints, limitations, computational power, budget, and data availability.\n",
        "Determine evaluation metrics for optimization, tracking KPIs, and required testing.\n",
        "Assess the model's relevancy to the target audience, focusing on prediction speed.\n",
        "Evaluate data availability and necessary features for collection.\n",
        "Define the scope of the solution to manage expectations.\n",
        "Consider deployment options such as cloud platforms, web apps, websites, or APIs.\n",
        "\n",
        "Step 2: **Data Collection**\n",
        "\n",
        "Identify reliable sources such as databases, APIs, sensors, or surveys.\n",
        "Specify the required data volume for effective analysis.\n",
        "Classify data as labeled or unlabeled based on availability.\n",
        "Address data quality issues, errors, bias, and consistency.\n",
        "Ensure data relevancy to the problem being addressed.\n",
        "Account for temporal effects and changes in the data.\n",
        "Handle legal and ethical concerns related to data privacy.\n",
        "Implement sampling strategies and data privacy techniques.\n",
        "Utilize appropriate tools for data collection.\n",
        "Implement version control to manage dataset changes.\n",
        "Consider continuous data collection for improved accuracy.\n",
        "\n",
        "Step 3: **Data Preprocessing**\n",
        "\n",
        "Handle missing values using various imputation techniques.\n",
        "Address outliers using standard deviation or IQR methods.\n",
        "Encode categorical variables using suitable techniques.\n",
        "Transform data through standardization, normalization, or other methods.\n",
        "Handle imbalanced datasets using techniques like oversampling or undersampling.\n",
        "Reduce dimensionality for better computational efficiency.\n",
        "Apply techniques to transform data for optimal model performance.\n",
        "\n",
        "Step 4: **Exploratory Data Analysis (EDA)**\n",
        "\n",
        "Analyze data distribution using summary statistics and visualizations.\n",
        "Explore relationships between variables through scatter plots and bar charts.\n",
        "Study complex interrelationships using heatmaps and pair plots.\n",
        "Identify temporal patterns and trends.\n",
        "Visualize categorical data using appropriate charts.\n",
        "Use PCA for dimensionality reduction and visualization.\n",
        "Perform statistical and hypothesis tests to validate assumptions.\n",
        "Visualize complex data types such as text or images.\n",
        "\n",
        "Step 5: **Model Selection, Training & Evaluation**\n",
        "\n",
        "Split data into training and testing sets.\n",
        "Choose suitable algorithms from a library based on the problem.\n",
        "Select evaluation metrics aligned with the problem domain.\n",
        "Ensure scalability and efficient processing for larger datasets.\n",
        "Optimize hyperparameters through techniques like grid search.\n",
        "Utilize parallel processing and GPU resources for training.\n",
        "Interpret and explain model decisions using tools like SHAP or LIME.\n",
        "Address imbalanced data to prevent bias in model performance.\n",
        "Consider pre-trained models and transfer learning for enhanced training.\n",
        "Implement early stopping to prevent overfitting.\n",
        "Save and load models for future use.\n",
        "Use experiment logging and versioning tools.\n",
        "Integrate the model into data processing pipelines.\n",
        "Implement feedback loops for retraining based on updated data.\n",
        "Explore automated machine learning (AutoML) for model selection and hyperparameter tuning.\n"
      ],
      "metadata": {
        "id": "F6v_1wHtG2nS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **GitHub Link -**"
      ],
      "metadata": {
        "id": "w6K7xa23Elo4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Provide your GitHub Link here."
      ],
      "metadata": {
        "id": "h1o69JH3Eqqn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Problem Statement**\n"
      ],
      "metadata": {
        "id": "yQaldy8SH6Dl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Problem Statement:\n",
        "Imagine yourself as a freelance data scientist ready for the next project adventure. Your task is to select a machine learning project from the list provided or propose an original project idea that resonates with you. Your objective is to identify a specific challenge within the chosen industry domain and design a machine learning solution to address it. Whether you're predicting customer behavior, optimizing processes, or making healthcare more efficient, your project should demonstrate your ability to approach complex problems, preprocess and analyze relevant data, develop and fine-tune models, and interpret results in a meaningful way. Your project will be a testament to your adaptability, curiosity, and aptitude for machine learning.\n"
      ],
      "metadata": {
        "id": "DpeJGUA3kjGy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ***Let's Begin !***"
      ],
      "metadata": {
        "id": "O_i_v8NEhb9l"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***1. Know Your Data***"
      ],
      "metadata": {
        "id": "HhfV-JJviCcP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Import Libraries"
      ],
      "metadata": {
        "id": "Y3lxredqlCYt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import Libraries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sb\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.svm import SVC\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn import metrics\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n"
      ],
      "metadata": {
        "id": "M8Vqi-pPk-HR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset Loading"
      ],
      "metadata": {
        "id": "3RnN4peoiCZX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load Dataset"
      ],
      "metadata": {
        "id": "4CkvbW_SlZ_R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "zg7uTf846fON"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv('/content/drive/MyDrive/TSLA.csv')"
      ],
      "metadata": {
        "id": "qIGns1MC7N3t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset First View"
      ],
      "metadata": {
        "id": "x71ZqKXriCWQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset First Look\n",
        "df.head()"
      ],
      "metadata": {
        "id": "LWNFOSvLl09H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "From the first five rows, we can see that data for some of the dates is missing the reason for that is on weekends and holidays Stock Market remains closed hence no trading happens on these days."
      ],
      "metadata": {
        "id": "NCmCV0bw7nWM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset Rows & Columns count"
      ],
      "metadata": {
        "id": "7hBIi_osiCS2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Rows & Columns count\n",
        "df.shape\n"
      ],
      "metadata": {
        "id": "Kllu7SJgmLij"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "From this, we got to know that there are 2416 rows of data available and for each row, we have 7 different features or columns."
      ],
      "metadata": {
        "id": "bRA8pOCd7zQ4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.describe()\n"
      ],
      "metadata": {
        "id": "w3Clcdxk75Dm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset Information"
      ],
      "metadata": {
        "id": "JlHwYmJAmNHm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Info\n",
        "df.info()\n"
      ],
      "metadata": {
        "id": "e9hRXRi6meOf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Exploratory Data Analysis**\n",
        "\n",
        "EDA is an approach to analyzing the data using visual techniques. It is used to discover trends, and patterns, or to check assumptions with the help of statistical summaries and graphical representations.\n",
        "\n",
        "While performing the EDA of the Tesla Stock Price data we will analyze how prices of the stock have moved over the period of time and how the end of the quarters affects the prices of the stock."
      ],
      "metadata": {
        "id": "duEewhVM8a1E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(15,5))\n",
        "plt.plot(df['Close'])\n",
        "plt.title('Tesla Close price.', fontsize=15)\n",
        "plt.ylabel('Price in dollars.')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "6Ze45qAg8j3K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The prices of tesla stocks are showing an upward trend as depicted by the plot of the closing price of the stocks."
      ],
      "metadata": {
        "id": "I_85UWPT8pgZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.head()\n"
      ],
      "metadata": {
        "id": "Ru1DoW8f8qVP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "If we observe carefully we can see that the data in the ‘Close’ column and that available in the ‘Adj Close’ column is the same let’s check whether this is the case with each row or not."
      ],
      "metadata": {
        "id": "PZnjKMkD8two"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df[df['Close'] == df['Adj Close']].shape\n"
      ],
      "metadata": {
        "id": "dGxqzKtH8v-5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "From here we can conclude that all the rows of columns ‘Close’ and ‘Adj Close’ have the same data. So, having redundant data in the dataset is not going to help so, we’ll drop this column before further analysis."
      ],
      "metadata": {
        "id": "0-loU4338yec"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = df.drop(['Adj Close'], axis=1)\n"
      ],
      "metadata": {
        "id": "QMCcpJfv80TC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now let’s draw the distribution plot for the continuous features given in the dataset.\n",
        "\n",
        "Before moving further let’s check for the null values if any are present in the data frame."
      ],
      "metadata": {
        "id": "6H4elL3f82uB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.isnull().sum()\n"
      ],
      "metadata": {
        "id": "Bb-XZzkX83bA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This implies that there are no null values in the data set provided."
      ],
      "metadata": {
        "id": "V5L0gzsx8705"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "features = ['Open', 'High', 'Low', 'Close', 'Volume']\n",
        "\n",
        "plt.figure(figsize=(20, 10))\n",
        "\n",
        "for i, col in enumerate(features):\n",
        "    plt.subplot(2, 3, i + 1)\n",
        "    sb.distplot(df[col])\n",
        "\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "LxSbx7CC8_NB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the distribution plot , we can see two peaks which means the data has varied significantly in two regions. And the Volume data is left-skewed."
      ],
      "metadata": {
        "id": "lb_8K4S09XSH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "features = ['Open', 'High', 'Low', 'Close', 'Volume']\n",
        "\n",
        "# Create a 2x3 grid of subplots with a specified size\n",
        "fig, axes = plt.subplots(2, 3, figsize=(20, 10))\n",
        "\n",
        "# Flatten the axes array for easier indexing\n",
        "axes = axes.flatten()\n",
        "\n",
        "for i, col in enumerate(features):\n",
        "    # Select the subplot for the current feature\n",
        "    ax = axes[i]\n",
        "\n",
        "    # Create a boxplot for the current feature\n",
        "    sb.boxplot(data=df, x=col, ax=ax)\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "bqQKm9HN9aoI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "From the above boxplots, we can conclude that only volume data contains outliers in it but the data in the rest of the columns are free from any outlier."
      ],
      "metadata": {
        "id": "0lmUffv69i4g"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Feature Engineering**"
      ],
      "metadata": {
        "id": "2gs0IFoa9lA9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Feature Engineering helps to derive some valuable features from the existing ones. These extra features sometimes help in increasing the performance of the model significantly and certainly help to gain deeper insights into the data."
      ],
      "metadata": {
        "id": "896qy7UO9oy2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "splitted = df['Date'].str.split('/', expand=True)\n",
        "\n",
        "# Print the structure of the 'splitted' DataFrame\n",
        "print(splitted)\n",
        "df.head()"
      ],
      "metadata": {
        "id": "nWbgxJws9hWe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['month'] = pd.to_datetime(df['Date']).dt.year\n",
        "df['is_quarter_end'] = np.where(df['month']%3==0,1,0)\n",
        "df.head()\n"
      ],
      "metadata": {
        "id": "A5ZpLYWjA3yr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "A quarter is defined as a group of three months. Every company prepares its quarterly results and publishes them publicly so, that people can analyze the company’s performance. These quarterly results affect the stock prices heavily which is why we have added this feature because this can be a helpful feature for the learning model."
      ],
      "metadata": {
        "id": "IgrrGy4-BK3Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Extract the year from the 'Date' column and create a new 'year' column\n",
        "df['year'] = pd.to_datetime(df['Date']).dt.year\n",
        "\n",
        "# Group the data by year and calculate the mean for each year\n",
        "data_grouped = df.groupby('year').mean()\n",
        "\n",
        "# Create subplots for 'Open', 'High', 'Low', and 'Close' columns\n",
        "plt.figure(figsize=(20, 10))\n",
        "\n",
        "for i, col in enumerate(['Open', 'High', 'Low', 'Close']):\n",
        "    plt.subplot(2, 2, i + 1)\n",
        "    data_grouped[col].plot.bar()\n",
        "    plt.title(col)\n",
        "\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "xbQXd5FjAj6L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "From the above bar graph, we can conclude that the stock prices have doubled from the year 2013 to that in 2014."
      ],
      "metadata": {
        "id": "k8gBn30oBODC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.groupby('is_quarter_end').mean()\n"
      ],
      "metadata": {
        "id": "ioTHKQ-bBSr0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here are some of the important observations of the above-grouped data:\n",
        "\n",
        "* Prices are higher in the months which are quarter end as compared to that of the non-quarter end months.\n",
        "\n",
        "* The volume of trades is lower in the months which are quarter end."
      ],
      "metadata": {
        "id": "UFUgvKlwBVFk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df['open-close'] = df['Open'] - df['Close']\n",
        "df['low-high'] = df['Low'] - df['High']\n",
        "df['target'] = np.where(df['Close'].shift(-1) > df['Close'], 1, 0)\n"
      ],
      "metadata": {
        "id": "BcoPXWV6BYyI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Above we have added some more columns which will help in the training of our model. We have added the target feature which is a signal whether to buy or not we will train our model to predict this only. But before proceeding let’s check whether the target is balanced or not using a pie chart."
      ],
      "metadata": {
        "id": "HuCPYH_uBbbY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(15, 6))\n",
        "plt.pie(df['target'].value_counts().values,\n",
        "\t\tlabels=[0, 1], autopct='%1.1f%%')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "dq7vi5uPBdHQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "When we add features to our dataset we have to ensure that there are no highly correlated features as they do not help in the learning process of the algorithm."
      ],
      "metadata": {
        "id": "xLte5zJvBvq0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(10, 10))\n",
        "\n",
        "# As our concern is with the highly\n",
        "# correlated features only so, we will visualize\n",
        "# our heatmap as per that criteria only.\n",
        "sb.heatmap(df.corr() > 0.9, annot=True, cbar=False,cmap='magma')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "EVipNuuABw-g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "From the above heatmap, we can say that there is a high correlation between OHLC that is pretty obvious, and the added features are not highly correlated with each other or previously provided features which means that we are good to go and build our model."
      ],
      "metadata": {
        "id": "ieqiLCY_CgYo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Data Splitting and Normalization**"
      ],
      "metadata": {
        "id": "Rsbxi_DbChq9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "features = df[['open-close', 'low-high', 'is_quarter_end']]\n",
        "target = df['target']\n",
        "\n",
        "scaler = StandardScaler()\n",
        "features = scaler.fit_transform(features)\n",
        "\n",
        "X_train, X_valid, Y_train, Y_valid = train_test_split(\n",
        "\tfeatures, target, test_size=0.1, random_state=2022)\n",
        "print(X_train.shape, X_valid.shape)\n"
      ],
      "metadata": {
        "id": "plPgFg08ClRA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "After selecting the features to train the model on we should normalize the data because normalized data leads to stable and fast training of the model. After that whole data has been split into two parts with a 90/10 ratio so, that we can evaluate the performance of our model on unseen data."
      ],
      "metadata": {
        "id": "lRGNrf3zCn4N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Model Development and Evaluation**"
      ],
      "metadata": {
        "id": "m4PNAn8ZCq1V"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now is the time to train some state-of-the-art machine learning models(Logistic Regression, Support Vector Machine, XGBClassifier), and then based on their performance on the training and validation data we will choose which ML model is serving the purpose at hand better.\n",
        "\n",
        "For the evaluation metric, we will use the ROC-AUC curve but why this is because instead of predicting the hard probability that is 0 or 1 we would like it to predict soft probabilities that are continuous values between 0 to 1. And with soft probabilities, the ROC-AUC curve is generally used to measure the accuracy of the predictions."
      ],
      "metadata": {
        "id": "f3Gj7BpvCwKs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "models = [LogisticRegression(), SVC(kernel='poly', probability=True), XGBClassifier()]\n",
        "\n",
        "for i in range(3):\n",
        "    models[i].fit(X_train, Y_train)\n",
        "\n",
        "    print(f'{models[i]} : ')\n",
        "    print('Training Accuracy : ', metrics.roc_auc_score(Y_train, models[i].predict_proba(X_train)[:,1]))\n",
        "    print('Validation Accuracy : ', metrics.roc_auc_score(Y_valid, models[i].predict_proba(X_valid)[:,1]))\n",
        "    print()\n"
      ],
      "metadata": {
        "id": "J0SlN7cKCydl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Among the three models, we have trained XGBClassifier has the highest performance but it is pruned to overfitting as the difference between the training and the validation accuracy is too high. But in the case of the Logistic Regression, this is not the case.\n",
        "\n",
        "Now let’s plot a confusion matrix for the validation data."
      ],
      "metadata": {
        "id": "NyCD-m58C9fs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Assuming you have a trained model (e.g., models[0])\n",
        "model = models[0]\n",
        "\n",
        "# Make predictions on the validation data\n",
        "y_pred = model.predict(X_valid)\n",
        "\n",
        "# Calculate the confusion matrix\n",
        "cm = confusion_matrix(Y_valid, y_pred)\n",
        "\n",
        "# Create a heatmap of the confusion matrix\n",
        "plt.imshow(cm, interpolation='nearest', cmap=plt.cm.Blues)\n",
        "plt.title('Confusion Matrix')\n",
        "plt.colorbar()\n",
        "\n",
        "classes = [0, 1]\n",
        "tick_marks = range(len(classes))\n",
        "plt.xticks(tick_marks, classes)\n",
        "plt.yticks(tick_marks, classes)\n",
        "\n",
        "plt.xlabel('Predicted')\n",
        "plt.ylabel('True')\n",
        "\n",
        "thresh = cm.max() / 2.\n",
        "for i in range(len(classes)):\n",
        "    for j in range(len(classes)):\n",
        "        plt.text(j, i, cm[i, j], horizontalalignment=\"center\", color=\"white\" if cm[i, j] > thresh else \"black\")\n",
        "\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "PlD2NMWTDBZ8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Conclusion**"
      ],
      "metadata": {
        "id": "gCX9965dhzqZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The stock market plays a remarkable role in our daily lives. It is a significant factor in a country's GDP growth. We can observe that the accuracy achieved by the state-of-the-art ML model is no better than simply guessing with a probability of 50%. Possible reasons for this may be the lack of data or using a very simple model to perform such a complex task as Stock Market prediction.\n",
        "\n",
        "* The project demonstrates the use of machine learning to predict stock price movements.\n",
        "* Further model tuning and evaluation metrics may be necessary to improve model performance."
      ],
      "metadata": {
        "id": "Fjb1IsQkh3yE"
      }
    }
  ]
}